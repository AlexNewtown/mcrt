\documentclass[a4paper, twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[pdftex, hidelinks,
            pdftitle={Monte Carlo Raytracing from Scratch},
            pdfauthor={Martin Estgren and Erik S. V. Jansson},
            pdfsubject={Rendering -- Global Illumination},
            pdfkeywords={rendering, global illumination, mcrt, c,
                         path tracing, photon mapping}]{hyperref}

\usepackage{bm}
\usepackage{caption}
\usepackage{listings}
\usepackage{booktabs}
\usepackage{mathtools}
\usepackage{subcaption}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{courier}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage[capitalize, noabbrev]{cleveref}
\usepackage[activate={true, nocompatibility}, final,
            tracking=true, kerning=true, spacing=true,
            factor=1100, stretch=10, shrink=10]{microtype}

\DeclareCaptionFormat{modifiedlst}{\rule{\textwidth}{0.85pt}\\[-2.9pt]#1#2#3}
\captionsetup[lstlisting]{format =  modifiedlst,
labelfont=bf,singlelinecheck=off,labelsep=space}
\lstset{basicstyle=\footnotesize\ttfamily,
        breakatwhitespace = false,
        breaklines = true,
        keepspaces = true,
        language = C++,
        showspaces = false,
        showstringspaces = false,
        frame = tb,
        numbers = left,
        numbersep = 5pt,
        xleftmargin = 16pt,
        framexleftmargin = 16pt,
        belowskip = \bigskipamount,
        aboveskip = \bigskipamount,
        escapeinside={<@}{@>}}

\title{\LARGE{\textbf{Monte Carlo Raytracing from Scratch}}}
\author{{\textbf{Martin Estgren}} \;\;\;\;\;\;\;\;\;\;\;\;\;\,   {\href{mailto:mares480@student.liu.se}
                                                                 {\texttt{<mares480@student.liu.se>}}} \\
        {\textbf{Rasmus Hedin}} \;\;\;\;\;\;\;\;\;\;\;\;\;\;\,\, {\href{mailto:rashe877@student.liu.se}
                                                                 {\texttt{<rashe877@student.liu.se>}}} \\
        {\textbf{Erik S. V. Jansson}} \;\;\;\;\;\;\;\;           {\href{mailto:erija578@student.liu.se}
                                                                 {\texttt{<erija578@student.liu.se>}}} \\~\\
        {Link√∂ping University, Sweden}\vspace{-1.0ex}}

\begin{document}
    \maketitle
    \section*{Abstract}

    A Monte Carlo raytracer is a renderer which produces photorealistic images after full convergence. It uses a global illumination model, meaning it gives optical phenomena like soft shadows, color bleeding and caustics. This report details the theory and the practical details necessary to implement one from scratch along with the photon mapping extension. After that, we benchmark the raytracer and display example renders from our raytracer. Finally, we do a discussion about the further improvements that can done in the raytracer and give some reflections on the project. The full \texttt{C++} source is on GitHub \footnote{https://github.com/CaffeineViking/mcrt}.

    \begin{figure}[ht]
        \centering
        \includegraphics[width=\linewidth]{share/new_render.png}
    \end{figure}

    \newpage \tableofcontents \clearpage

    \section{Introduction} \label{sec:introduction}

        Several fields of industry use \emph{computer graphics} to generate and display synthetic images on a screen; e.g.\ the entertainment industry uses \emph{raytracers} for \emph{rendering} animated movies while \emph{rasterizers} usually are the technology powering real-time video games. Of course, it's also widely used in the engineering and scientific disciplines for visualizing field data, which even have their own sub-field called \emph{scientific visualization}. Since it is such a wide field, we'll only be focusing on the \emph{rendering problem}: the task of converting one \emph{scene description} to an \emph{image} of it.

        Rendering can usually be done in one of two ways, called the \emph{rasterization} and \emph{raytracing} techniques, or, by using some hybrid of these. \emph{Rasterization} is when we geometrically project a scene, composed of primitives, onto an image plane (our camera) and then color the pixels based on a \emph{local lighting model}. Meaning, objects in a scene are \emph{shaded} only based on position, material properties, viewpoint direction, and light source information; never on other objects. Rasterization is very fast since there is hardware dedicated to these operations, and each of these is independent of each other, in other words, it's an \emph{embarrassingly parallel} problem. \emph{Raytracing} on the other hand shoots \emph{rays} from the pixels in the \emph{camera viewplane} and finds \emph{intersections} with geometry in the scene. These rays bounce around the scene by \emph{specular reflection} or \emph{specular transmission}, until it finds a \emph{diffuse surface}, which then \emph{absorbs} it. It's a process by recursion, which approximates \emph{irradiance} falling onto a pixel. Since it takes into account other objects, it's a technique which gives \emph{global illumination}. Unfortunately, raytracing doesn't have full hardware support, but is still fast since it's also an embarrassingly parallel task (done for each sample).

        In this report we'll describe all the party tricks we've used to implement our \emph{Monte Carlo raytracer} from scratch. It simulates all \emph{light transport} effects for \emph{perfectly diffuse} and \emph{perfectly specular} surfaces. We've also implemented \emph{photon mapping} to speed up our convergence rate for higher quality \emph{caustics}. Arbitrary \emph{triangle meshes} can also be rendered and use \emph{bounding volumes} to ignore low-effort intersection tests. Lastly, we also support \emph{quadric geometry}. All of our scene is specified by using a JSON format.

        We continue \cref{sec:introduction} by giving a brief overview of the field and introducing desirable properties for achieving \emph{photorealism} with a model describing it. We then take a excursion through the most notable rendering schemes to solve the \emph{rendering equation}. In \cref{sec:theory_and_method} we break apart our raytracer bit-by-bit and explain each part in turn, along with any relevant theory necessary. We then play around with our raytracer's knobs in \cref{sec:results_and_benchmark} to see if they affect the final render, and more importantly, the rendering time. We'll try to see if there is a balanced trade-off between render quality and speed. After all this, \cref{sec:discussion_and_outlook} concludes the report by critically looking at our implementation and results and also giving insight into what could be improved further.

    \begin{figure}[ht]
        \centering
        \begin{subfigure}{0.48\linewidth}
            \centering
            \label{fig:cornell_box_local_illumination}
            \includegraphics[width=\linewidth]{share/cornell_local_illumination.png}
            \caption{local illumination}
        \end{subfigure} \hfill
        \begin{subfigure}{0.48\linewidth}
            \centering
            \label{fig:cornell_box_global_illumination}
            \includegraphics[width=\linewidth]{share/cornell_global_illumination.png}
            \caption{global illumination}
        \end{subfigure}
        \caption{Two renders of the famous \emph{Cornell box} in our raytracer; comparing both illumination models.}
        \label{fig:cornell_box}
    \end{figure}

    \vspace{-1.5em}

    \subsection{Global\, Illumination} \label{sec:global_illumination}

    A rendering technique is deemed to be \emph{photorealistic} if, after full convergance, it exactly mimics reality. One of the most important ingredients for achieving photorealism is \emph{global illumination}. We can compare \emph{local illumination} and \emph{global illumination} by looking at \cref{fig:cornell_box}. Notice that GI (global illumination) has \emph{color bleeding}, \emph{hard and soft shadows} and \emph{caustics}. These phenomena are not possible in a local model without ``hacks'' like the \emph{shadow mapping} technique.

    \subsection{Rendering Equation} \label{sec:rendering_equation}

        A model describing most light transport phenomena is the \emph{rendering equation}, shown below, presented by \emph{James Kajiya}~\cite{kajiya1986rendering}. All of the rendering schemes are attempts at solving it. We present in the coming sections the most well known rendering techniques. \begin{align*}
            \mathcal{L}_o(\vec{x}, \hat{\omega}_o) &= \mathcal{L}_e(\vec{x}, \hat{\omega}_o) \; +\\
                                                   &+ \int_\Omega \mathcal{L}_i(\vec{x}, \hat{\omega}_i)
                                                      f_r(\vec{x}, \hat{\omega}_i, \hat{\omega}_o)
                                                      (\hat{n}_{x} \cdot \hat{\omega}_i) \, d\hat{\omega}_i
        \end{align*}

    \subsection{Radiosity} \label{sec:radiosity}
    \textit{Radiosity} for computer graphics assumes all surfaces are \emph{perfectly diffuse patches} (e.g. triangles), and builds on the concept of finding the radiance that is transferred between patches in the scene using an iterative process. Each iteration solves the equation below by reflecting radiance for each patch: \begin{equation*}
        B_i = E_i + \rho_i \sum_{j=1}^{n} F_{ij} B_j \; ,
    \end{equation*}
    where \(B_i\) is the \textit{radiosity} of \emph{patch} \(i\), \(E_i\) the \emph{emitted radiosity} by \(i\), \(\rho_i\) is the \textit{surface reflectivity} of \(i\), and the sum defines the \emph{radiance} falling onto \(i\) between each patch in the scene and the patch \(i\), depending on the \emph{form-factor} \(F_{ij}\) (geometric term) of \(i\) and \(j\).

    \begin{figure}[ht]
        \centering
        \includegraphics[width=\linewidth]{share/radiosity.png}
        \caption{Shows how the radiosity technique spreads the radiance in each iteration (render by \emph{H. Elias}).}
        \label{fig:radiosity}
    \end{figure}

    \vspace{-0.5em}

Iterating on the above equations, a discrete approximation of the \textit{rendering equation} is achieved for perfect Lambertian surfaces as the patch size goes infinitesimal and the iterations tends to infinity (but there needs to be some cutoff point, otherwise numerical errors might start appearing). You can see a radiosity-based renderer iterate through a scene in \cref{fig:radiosity}. Some advantages of radiosity over GI techniques that takes into account specular light is that it calculates the radiosity of the full scene and not just based on the camera viewport, making it useful when baking light-maps with the scene. It's a spin-off of the heat transfer equations in thermodynamics into rendering by \emph{Goral et al.}~\cite{goral1984modeling}.

    \subsection{Whitted Raytracing} \label{sec:whitted_raytracing}

    In the \emph{Whitted}~\cite{whitted1980improved} ray tracing method initial rays are emitted from the camera into the scene. The rays will intersect with objects and new reflective and/or refractive rays are spawned. In this method it is assumed that we only have \emph{perfect reflections and refractions}. The direction of a refracted ray is computed with Snell's law given the refractive indices of the materials at the ray intersection point.

    Reflecting and refracting rays will build up a \emph{tree of rays}. When all rays have terminated (i.e. hit a diffuse surface) the radiance at each intersection is computed from the leaf nodes back up to the root node (the camera). The radiance from the child rays of an intersection point contribute to the total radiance leaving each intersection point. \emph{Shadow rays} are sent from each of these points to the light sources, to know if the point is in shadow. If the shadow ray reaches a source of light then it's not in shadow and the light therefore contributes to the total radiance. \cref{fig:raytracing} (a) is being rendered by it.

    \begin{figure}[ht]
        \centering
        \begin{subfigure}{0.48\linewidth}
            \centering
            \label{fig:whitted_raytracing}
            \includegraphics[width=\linewidth]{share/whitted_raytracing.png}
            \caption{Whitted-ish raytracing}
        \end{subfigure} \hfill
        \begin{subfigure}{0.48\linewidth}
            \centering
            \label{fig:monte_carlo_raytracing}
            \includegraphics[width=\linewidth]{share/monte_carlo_raytracing.png}
            \caption{Monte Carlo raytracing}
        \end{subfigure}
        \caption{Renders of \emph{the scene} in \cref{sec:scene_description} using our raytracer in the early and later progress stages.}
        \label{fig:raytracing}
    \end{figure}

    \vspace{-1.5em}

    \subsection{Path Tracing} \label{sec:path_tracing}

    The \emph{path tracing} method works in a similar way to Whitted raytracing, but the reflections and refractions may not always perfect. Instead, when a ray intersects with an object it will be reflected in a \emph{random direction} originating from the \emph{hemisphere} at the intersection point. These rays are recursively reflected until they intersect with a light source. But since some rays never hit a light source another condition to terminate is needed. The solution is called \emph{Russian roulette}, where rays have a probability of being \emph{absorbed} at the intersection point. This gives the technique an unbiased result. A tree will be built recursively just as before, adding contribution from the child nodes and from the light sources (which may have an area now) with a local lighting model.

    This is a method that needs many \emph{samples} to converge, and thus uses \emph{Monte Carlo integration}; making the \emph{path tracing technique} a \emph{Monte Carlo raytracer}. \cref{fig:raytracing} (b) shows a render of a scene using our Monte Carlo raytracer; we have more interesting light phenomena than Whitted raytracing.

    \subsection{Photon Mapping} \label{sec:photon_mapping}

    One of the main problems with \textit{path tracing} is that it usually takes very long to \emph{converge} to a \emph{noise-less} image. To speed up the raytrace convergence, \emph{H. Jensen} proposed an algorithm where rays with low importance are approximated using a spatial map which describe how light sources deposit flux in the scene. These data-structures are called \textit{photon maps}~\cite{jensen1996global} and significantly speed up convergence of a typical \textit{path tracer} by requiring a bit more memory.

    \vspace{0.5em}

    \begin{figure}[ht]
        \centering
        \begin{subfigure}{0.478\linewidth}
            \centering
            \label{fig:cornell_box_photon_map}
            \scalebox{-1}[1]{\includegraphics[width=\linewidth]{share/photon_mapping.png}}
            \caption{direct light photon map}
        \end{subfigure} \hfill
        \begin{subfigure}{0.498\linewidth}
            \centering
            \includegraphics[width=\linewidth]{share/cornell_global_illumination.png}
            \caption{possible raytrace result}
        \end{subfigure}
        \caption{Here the flux coming from the light source is distributed as seen in (a) before we do raytracing.}
        \label{fig:photon_mapping}
    \end{figure}

    \vspace{-0.25em}

    \textit{Photon mapping} is done in two passes. The first pass is done before the \textit{path tracer} as a pre-processing step where photons are emitted from light sources and bounced around the scene in a way similar to the typical \textit{path tracer}. These photons are usually stored in a \emph{balanced kd-tree} whenever they hit a diffuse surface. The second pass is done in conjunction with the \textit{path tracer} where rays with low importance are approximated by photons stored around the terminating scene-intersection. The number of photons to consider is either calculated by a bounding sphere of fixed size or a sphere expanded until it encompasses a fixed number of photons. These photons are then used to approximate radiance at the intersection point. We'll see later in the report that our implementation uses the ``\emph{fixed sphere radiance estimation}'' approach for this.

In some cases a higher resolution secondary \textit{photon map} is used to show caustics effects since these are very expensive to do in regular \textit{path tracing}. In this case, photons are only emitted towards specular objects in the scene and in much greater numbers compared to the regular \textit{photon map}. This enables one to get detailed caustic effects cheaply and fast.

    \section{Theory and Method} \label{sec:theory_and_method}

        Raytracing attempts to find irradiance falling onto each pixel in the \emph{image plane} \(E_\Pi\). For each pixel at world position \(\vec{o}_{ij}\) on \(E_\Pi\) we send off \emph{rays} originating at \(E\), the \emph{eye origin}. These \emph{importance rays} will \emph{hit} and \emph{bounce} around the \emph{scene} described in \cref{sec:scene_description} according to the theory at \cref{sec:ray-surface_intersections}. Depending on the surface properties, we'll need to consider the \emph{radiance reflectance rate}, \(f_r(\vec{x}, \hat{\omega}_i, \hat{\omega}_o)\), in \cref{sec:surface_properties}, for evaluating the \emph{radiance} at e.g. the point \(\vec{p}\). The \emph{light contributions} for \(\vec{p}\) come in two forms: \emph{direct light} (e.g. the light source \(L\)) and \emph{indirect light} (e.g. the diffuse surface \(D\)). These are described in Sections \ref{sec:direct_light_contributions} and \ref{sec:indirect_light_contributions}. To reduce noise, and converge faster, we use \emph{photon maps} in Part~\ref{sec:photon_mapping}, to approximate the direct light contributions. Lastly, we may get \emph{aliasing}, which we deal with in Part~\ref{sec:anti-aliasing_and_sampling}.

        \begin{figure}[ht]
            \centering
            \includegraphics[width=0.8\linewidth]{share/raytracing_overview.eps}
            \label{fig:raytracing_overview}
        \end{figure}

        \vspace{-1.8em}

        \subsection{Scene Description} \label{sec:scene_description}

        Below is an overview of the scene we'll be using to display the results and do our benchmarking. All walls are Lambertian reflectors with different colors, while the floor and roof are white. An area light source with 4 triangles exists, it's white, on the roof.

        There exists four primitives in the scene, an orange teapot mesh (4) with a Lambertian BRDF, a white sphere (2) with Oren-Nayar BRDF \(\sigma' = 0.1\), a perfectly reflective sphere in (3) and a perfectly refractive sphere in (1) with a refraction index 1.5.

        \begin{figure}[ht]
            \centering
            \includegraphics[width=0.8\linewidth]{share/scene_description.eps}
            \label{fig:scene_description}
        \end{figure}

        \vspace{-1.5em}

        \clearpage

        \subsection{Ray-Surface Intersections} \label{sec:ray-surface_intersections}

        The scene can be constructed using a number of different geometric surfaces and requires different intersection test for each surface type. Common for them all is that the incoming ray is defined on the parametric formula \( \vec{o} + t \hat{d} \) where \( \vec{o} \in \mathbb{R}^3\) denotes the origin of the ray, \(t \in \mathbb{R}\) specifies a distance from the origin, and \(\hat{d} \in \mathbb{R}^3\) indicates the direction of the ray.
            
        \subsubsection{Parametric Sphere} \label{sec:parametric_sphere}
        
        Spherical objects are defined as a vector \(\vec{o} \in \mathbb{R}^3\) which denotes the origin of the sphere, and a scalar \(r\) as the radius of the sphere. Sphere-ray intersections are done using a geometric solution to the following:

        \begin{align*}
            \vec{L} = \vec{o_{sphere}} - \vec{o_{ray}} \\
            t_{ca} = \vec{L} \cdot  \hat{d_{ray}} \\
        \end{align*}
        if \(t_{ca} < 0\), there's no intersection between ray and sphere. Otherwise look for intersection points using:
        
        \begin{align*}
            d^2 = L \cdot L - t_{ca} \cdot t_{ca}
        \end{align*}
        if \(d^2 > r^2\), there's not intersection, otherwise:

        \begin{align*}
            t_{hc} &= \sqrt{r^2 - d^2} \\
            t_0 &= t_{ca} - t_{hc} \\
            t_1 &= t_{ca} + t_{hc} \\
            t_{min} &= min(t_0, t_1)
        \end{align*}
        where \(t_{min}\) is the distance from \(\vec{o_{ray}}\) to the closest intersection point on the sphere. If \(t_{min}  < 0\) we don't have an intersection as well. The intersection point is calculated as  \(\vec{i} = \vec{o_{ray}} + t_{min} \hat{d_{ray}} - o_{sphere} \) and the surface normal as \( \hat{n} = || \vec{i} - \vec{o_{sphere}} || \).

        \subsubsection{Triangle Polygon} \label{sec:triangle_polygon}
        
        Intersections between triangles and rays are computed using the \textit{M√∂ller-Trumbore}~\cite{moller2005fast} algorithm, which allow for intersection tests without having to compute plane containing the triangle. First a local coordinate system for the triangle is computed using the corner vertices \(\vec{v_1}\), \(\vec{v_2}\), and \(\vec{v_3}\) of the form:

\begin{align*}
    \hat{e_1} = \vec{v_2} - \vec{v_1} \\
    \hat{e_2} = \vec{v_3} - \vec{v_1}
\end{align*}

the surface normal is extracted as \( \hat{n} =\hat{e_1} \times \hat{e_2} \), we check if the ray travels parallel with the triangle surface by \(\hat{p} = \hat{d_{ray}} \times \hat{e_2}\), and if the determinant  \(d = \hat{e_1} \cdot \hat{p}\), \(d = 0 \), the ray does not intersect the triangle. Otherwise the intersection test continues:

\begin{align*}
    \vec{t} &= \vec{o_{ray}} - \vec{v_1} \\
    \vec{q} &= \vec{t} \times \hat{e_1} \\\\
    u &= \vec{t} \cdot \hat{p} * d^{-1} \\
    v &= \hat{d_{ray}} \cdot \vec{q} * d^{-1}
\end{align*} 
if the two conditions
 
\begin{align*}
    u < 0 \; &\textbf{and} \; u > 1 \\
    v < 0 \; &\textbf{and}  \; u + v > 1 
\end{align*}
holds we have an intersection and the distance \(t\) the ray has to travel to the intersection point can be calculated as \(\vec{e_2} \cdot \vec{q} * d^{-1} \).

        
        \subsubsection{Triangle Mesh} \label{sec:triangle_mesh}
        
        To speed up the intersection test for geometries using multiple triangles we use a hierarchical geometric representation where the geometry of interest is encapsulated within a bounding sphere, this speeds up the test since only the triangles are taken into account if we already know that the ray will intersect the bounding sphere.

    The bounding sphere is calculated by picking the smallest and largest \textit{vertecies} from the vertex set \(\mathcal{V}\)
    \begin{align*} 
        \vec{v_{min}} &= \underset{\vec{v} \in \mathcal{V}}{argmin}(\vec{v}) \\
        \vec{v_{max}} &= \underset{\vec{v} \in \mathcal{V}}{argmax}(\vec{v})
    \end{align*}
    and defining the bounding sphere as:
    \begin{align*}
        radius &= \frac{ || \vec{v_{min}} - \vec{v_{max}} ||}{2} \\
        \vec{origin} &= \frac{\vec{v_{max}} - \vec{v_{min}}}{2}
    \end{align*}

        \subsection{Surface Properties} \label{sec:surface_properties}

            After hitting a diffuse surface with normal \(\hat{n}\) coming from a ray direction \(\hat{\omega}_i\), we need to find the reflected radiance going towards \(\hat{\omega}_o\). This is affected by the \emph{bidirectional reflectance distribution function}, \(f_r\), of the surface. It's used to model the \emph{ratio} of \emph{reflected radiance} leaving the surface towards \(\hat{\omega}_o\) with respect to the \emph{irradiance} arriving from the direction \(\hat{\omega}_i\). The first definition given by \emph{F. E. Nicodemos}~\cite{nicodemus1965directional} shows: \[f_r(\vec{x}, \hat{\omega}_i, \hat{\omega}_o) = \frac{d \mathcal{L}_{r}(\vec{x}, \hat{\omega}_o)}{\mathcal{L}_i(\vec{x}, \hat{\omega}_i)(\hat{n} \cdot \hat{\omega}_i)d\hat{\omega}_i}\; .\]

            There are several \emph{surface reflectance models}; we'll only describe the \emph{Lambertian} and the \emph{Oren-Nayar} \emph{reflectance models}, since those are in our raytracer.

            \subsubsection{Lambertian Model} \label{sec:lambertian_model}

                Assumes the surface reflects radiance \emph{isotropically}, meaning it reflects radiance \emph{equally in all directions}. It was introduced by \emph{J. H. Lambert}~\cite{lambert1760photometria} in 1760, and realistically models perfectly diffuse surfaces which are free of imperfections (i.e. there is no roughness).

                \begin{figure}[ht]
                    \centering
                    \includegraphics[width=0.8\linewidth]{share/lambertian_model.eps}
                    \caption{Diffuse ``Lambertian'' surface.}
                    \label{fig:lambertian_model}
                \end{figure}

                Since it doesn't depend on the incoming/outgoing directions, the value of \(f_r\) is constant in all directions within the hemisphere. This gives us the BRDF in \cref{eq:lambertian_model}. Now, usually you see the factor \(\hat{n} \cdot \hat{\omega}_i\) here as well, but we've chosen to not include it since it'll be appearing in the rendering equation anyway.

                \begin{equation} \label{eq:lambertian_model}
                    f_r(\vec{x}, \hat{\omega}_i, \hat{\omega}_o) = \frac{\rho}{\pi} \; ,
                \end{equation} where \(\rho\) is the \emph{albedo} of the diffuse surface \(D\) and \(\pi\) is the \emph{normalization factor} for \emph{energy conservation}. \emph{Albedo} can hand-wavingly be interpreted as ``color''.

            \subsubsection{Oren-Nayar Model} \label{sec:oren-nayar_model}

                Unfortunately, while the Lambertian model might be good for some surface types, it's inappropriate for others, such as concrete, ceramic and cloth. Which is a shame, since it's an simple and intuitive model.

                Surfaces which are diffuse and have rough texture can be accurately modeled with the \emph{Oren-Nayar}~\cite{oren1994generalization} \emph{reflection model}. It's based on the \emph{microfacet model} introduced by \emph{Torrace-Sparrow}~\cite{torrance1967theory}, which assumes a surface is composed of infinitesimally small, flat, Lambertian reflectors (as shown in the last section). An intuitive sketch of such a surface can be seen in \cref{fig:oren-nayar_model}, where the V-shaped microfacets are actually supposed to be infinitesimally small sized.

                \begin{figure}[H]
                    \centering
                    \includegraphics[width=0.8\linewidth]{share/oren-nayar_model.eps}
                    \caption{Diffuse ``Oren-Nayar'' surface.}
                    \label{fig:oren-nayar_model}
                \end{figure}

                Deriving \cref{eq:oren-nayar_model} is outside the scope of this paper, so if you're interested, consult the original paper. Note that this is the \emph{qualitative Oren-Nayar} definition, and will therefore fail energy conservation laws when the \emph{roughness} parameter is set \(\sigma > 0.97\).

                \begin{equation} \label{eq:oren-nayar_model}
                    \begin{split}
                        f_r(\vec{x}, \hat{\omega}_i, \hat{\omega}_o) = \frac{\rho}{\pi} \cdot (A + \sin\alpha \cdot \tan\beta \cdot \gamma \cdot B) \; ,\\
                        A = 1 - 0.5 \frac{\sigma^2}{\sigma^2 + 0.33} \; , \; B = 0.45\frac{\sigma^2}{\sigma^2 + 0.09} \; ,\\
                        \alpha = \max\, \{\theta_i, \theta_o\} \; , \; \beta = \min\, \{\theta_i, \theta_o\} \; ,\\
                        \gamma = \max\, \{0, \phi_i - \phi_o\} \; ,
                    \end{split}
                \end{equation} where \(\theta\) is the \emph{inclination} and \(\phi\) the \emph{azimuth} of \(\hat{\omega}\). By changing \(\sigma\) we can control the \emph{surfaces' roughness}. Notice that this BRDF dependents on the ray angle, and is therefore \emph{anisotropic}. Implementing this isn't that straightforward since our raytracer has \(\hat{\omega}\)'s that aren't easily decomposable into the form \(\hat{\omega} = (\theta, \phi)\). It boils down to doing projections onto the surface plane. We can get a vector parallel to the plane with \(\hat{p} = \hat{\omega}_i \times \hat{\omega}_o\) and then \(\hat{q} = \hat{p} \times \hat{n}\). By projecting down \(\hat{\omega}_i\) onto \(\hat{q}\) with \(\hat{\omega}_{i\parallel \hat{q}}\), we get \(\theta = \hat{\omega}_i \cdot \hat{q} \; , \; \phi = \hat{\omega}_{i\parallel \hat{q}} \cdot \hat{q}\).

                Because the qualitative Oren-Nayar calculations have quite a few expensive operations, we've chosen to implement \emph{Yasuhiro Fujii's}~\cite{fujii2015improvement} Oren-Nayar variant. It doesn't require trigonometric functions, and gives empirically almost indistinguishable results to O-N. Like regular Oren-Nayar, it suffers from not following energy conservation laws when \(\sigma' > 0.97\).

                Actually, experimentally, \emph{Y. Fujii} has found that his proposed approach matches regular Oren-Nayar better than the qualitative Oren-Nayar found in \cite{oren1994generalization}.

                \vspace{-1em}

                \begin{equation} \label{eq:improved_oren-nayar_model}
                    \begin{split}
                        f_r(\vec{x}, \hat{\omega}_i, \hat{\omega}_o) = \rho \cdot \Big(A' + \frac{s}{t} \cdot B'\Big) \; ,\\
                        A' = \frac{1}{\pi + \big(\frac{\pi}{2} - \frac{2}{3}\big) \sigma'} \; , \; B' = \frac{\sigma'}{\pi + \big(\frac{\pi}{2} - \frac{2}{3}\big) \sigma'} \; ,\\
                        t = \begin{cases}
                            1 & \textbf{if}\;\;\; s \leq 0\\
                            \max\, \{\hat{n} \cdot \hat{\omega}_i, \hat{n} \cdot \hat{\omega}_o\} & \textbf{otherwise}
                        \end{cases} \; ,\\
                        s = \hat{\omega}_i \cdot \hat{\omega}_o - (\hat{n} \cdot \hat{\omega}_i)(\hat{n} \cdot \hat{\omega}_o) \; .
                    \end{split}
                \end{equation}

                Unlike qualitative Oren-Nayar, the above BRDF is a breeze to evaluate, for both the hardware and the programmer (who likes projecting stuff anyway).

                Looking at \cref{fig:roughness} we see results returned from our raytracer. We should make a couple of interesting observations. Notice that Oren-Nayar reflectors with \(\sigma' = 0\) are essentially the same as Lambertian reflectors. Which makes sense, since \(\sigma' = 0\) means the surface has no roughness (i.e. perfectly diffuse).

                \begin{figure}[ht]
                    \centering
                    \includegraphics[width=\linewidth]{share/roughness.png}
                    \caption{Spheres rendered using our raytracer with different roughness values, \(\sigma'\), for the Oren-Nayars.}
                    \label{fig:roughness}
                \end{figure}

                To wrap things up, when we hit a diffuse surface, we wish to know the ratio of radiance that will leave the surface. For this we need to choose between the Lambertian and Oren-Nayar model to calculate \(f_r\). Each surface in our scene defs. it's \(f_r\) model to use.

        \subsection{Direct Light Contributions} \label{sec:direct_light_contributions}
        Most importance rays that are sent from the camera into the scene never hit a light source. Therefore, the direct light from the light sources to an intesection point needs to contribute to the total radiance at the point. This is computed with \emph{shadow rays}, which are sent from each intersection point to the sources of light. The shadow rays will provide light contributions to the radiance at the ray intersection points. To determine if the intersection point lies in shadow, a visbility test is performed. Transparent objects are obviously ignored by the visibility test.
            \subsubsection{Point Light Source} \label{sec:point_light_source}
            The shadow ray $S_2 = x - y$ from an intersection point $y$ to a point light source position $x$ is launched. If it does not hit any object, it will pass the visibility test, and will therefore contribute radiance to the surface. The Lambertian cosine falloff, based on the angle $\theta_{S_2}$ between the normal $N$ and $S_2$, is applied, as can be seen in the equation below.

            \begin{equation*}
              L(y \rightarrow -\Psi_1) = f_r(y, \omega_{S_2}, -\Psi_1) L(y \leftarrow S_2) cos\theta_{S_2}
            \end{equation*}
            
            \begin{figure}[ht]
              \centering
              \includegraphics[width=0.9\linewidth]{share/shadow_ray.eps}
              \caption{Shadow ray toward point light.}
            \end{figure}

            \subsubsection{Area Light Source} \label{sec:area_light_source}
            The direct light from an area light source needs to be approximated for each intersection point. This is done with a Monte-Carlo estimator $<L_D>$ using $M$ shadow rays, as will be explained in the next section. The light source is a triangle with corners $\mathbf{v_0}$, $\mathbf{v_1}$, and $\mathbf{v_2}$ and with area $A = 0.5 ||(\mathbf{v_1} - \mathbf{v_0})\times(\mathbf{v_2}-\mathbf{v_0})||$. To get the shadow rays we need to pick random points $q$ on the light source uniformly with PDF $p(q) = \frac{1}{A}$. This is done by drawing two random numbers $u, v$ until $u + v < 1$ and building the point $q$ with barycentric coordinates $q = (1-u-v)\mathbf{v_0} + u\mathbf{v_1} + v\mathbf{v_2}$. Repeat this until we have $M$ points. 


            \subsubsection{Monte Carlo Method} \label{sec:monte_carlo_method}
            \begin{equation*}
              <L_D> = \frac{AL_0}{M}\sum_{i=1}^{M}{f_r V(x, q) G(x, q_i)}
            \end{equation*}
             $G(x, q) = cos\alpha cos\beta / d^2$ is the geometric term where d is the length of the shadow ray and $\alpha$, $\beta$ are the inclination angles to the surface normal at the start- and endpoint. $V(x, q_i)$ is the visibility function which is 1 if no object is in the way and 0 otherwise. Thus, only unblocked shadow rays will contribute to the estimate giving a soft shadow. Since the light source is a Lambertian emitter $L_0$ is emitted for all points and directions of the light source.
            
            \subsection{Indirect Light Contributions} \label{sec:indirect_light_contributions}
            When rays are emitted from the camera they will intersect with surfaces. At the intersections the rays will either terminate or spawn new rays depending on the surface material. By following the path of the ray we build up a tree with intersections as nodes connected by rays. The child nodes will contribute radiance to the parent nodes which gives indirect light, i.e. light reflected at least once since it left the light source. It's usually split it into three types:
            \subsubsection{Specular Reflection} \label{sec:specular_reflection}
            If a ray hits a perfect specular surface a new ray will always spawn. The direction of the new ray is determined by the perfect specular reflection law. The direction of the new ray $\vec{R} = \vec{I} - 2 (\vec{I} \cdot \vec{N}) \vec{N}$ where $\vec{I}$ is the incoming ray and $\vec{N}$ is the normal.

            \subsubsection{Specular Refraction} \label{sec:specular_refraction}
            If a ray hits a perfectly refractive surface, i.e. a transparent object, a new ray is spawned that enters the object.
            The refractive ray $\vec{T}$ is computed with the perfect refraction law and the angle to the surface normal is computed with \emph{Snell's law}.

            \small
            \begin{equation*}
              \vec{T} = \tfrac{n_1}{n_2}\vec{I} + \vec{N}\left(-\tfrac{n_1}{n_2}(\vec{N}\cdot\vec{I}) - \sqrt{1 - [\tfrac{n_1}{n_2}]^2[1 - (\vec{N}\cdot\vec{I})^2]}\right)
            \end{equation*}
            \normalsize

            When a ray is moving between media of different refractive indices we might both reflect and refract. The Fresnel equations are used to determine the distribution of the radiance over the refracted and reflected rays.

            \begin{equation*}
            R_s = \left[\frac{n_1cos\theta_1 - n_2\sqrt{1-([n_1/n_2]sin\theta_1)^2}}{n_1cos\theta_1 + n_2\sqrt{1-([n_1/n_2]sin\theta_1)^2}}\right]^2
          \end{equation*}

          \begin{equation*}
            R_p = \left[\frac{n_2\sqrt{1-([n_1/n_2]sin\theta_1)^2} - n_1cos\theta_1}{n_2\sqrt{1-([n_1/n_2]sin\theta_1)^2} + n_1cos\theta_1}\right]^2
          \end{equation*}

          The total reflection coefficient becomes
          \begin{equation*}
            k_r = (R_s + R_p)/2
          \end{equation*}

          The radiance of the parent node to the refracted and reflected ray is computed as
          \begin{equation*}
            L = R k_r + T (1-k_r)
          \end{equation*}

          \subsubsection{Diffuse Reflection} \label{sec:diffuse_refraction}
          As described in \ref{sec:surface_properties} two diffuse surfaces are implemented in the project; Lambertian and Oren-Nayar. When these surfaces are intersected by a ray we need to determine the direction of the reflected ray. For specular surfaces the ray was reflected/refracted perfectly, but for diffuse surfaces this is not the case. Instead, a random azimuth angle $\phi_i$ and inclination angle $\theta_i$ is picked in the hemisphere. We pick two random values $u, v \in [0,1)$ and compute azimuth $\phi_i = 2\pi u$ and inclination $\theta_i = cos^{-1}(\sqrt{v})$. The reflected ray will contribute to the radiance at the intersection giving indirect light.

          In order to not get a biased result an unbiased termination condition is required. This is achieved with russian roulette \ref{sec:russian_roulette} that determines if a ray should be terminated.

          \subsubsection{Russian Roulette} \label{sec:russian_roulette}
          In order to achieve a unbiased result russian roulette is used to get an unbiased termination condition. The idea is to select a reflection probability $P$ for an intersection of a ray. This probability is connected to the surface properties. The radiance at a intersection point is approximated with a Monte-Carlo scheme using one ray.

          \begin{equation*}
            L(x \rightarrow \omega_{out}) = \pi f_r(x, \omega_{in}, \omega_{out}) L(x \leftarrow \omega_{in})
          \end{equation*}

          A new ray is spawned at the intersection point with probability $P$ and the ray is terminated with probability $(1-P)$. In order to remove the bias the following equation is used.

          \begin{equation*}
            L(x \rightarrow \omega_{out}) = \frac{\pi}{P} f_r(x, \omega_{in}, \omega_{out}) L(x \leftarrow \omega_{in})
          \end{equation*}

          To determine if a ray should be terminated the azimuth angle of the reflected ray is modified to $\phi_i = (2\pi/P)u$ where $u$ is a random variable $0 \leq u \leq 1$. If $\phi_i > 2\pi$ the ray is terminated and a new ray is spawned if $0 \leq \phi_i \leq 2\pi$.

        \subsection{Photon Mapping} \label{sec:photon_mapping}

        Photon mapping is a technique where photons are distributed within the scene and used during the path tracing render pass. This is done in order to provide an approximative method for radiance sampling during the path tracing step. Each photon represents a tiny packet of flux which provides information that helps us to reduce the number of reflection and shadow rays during the path tracing pass. 

            \subsubsection{Gathering Photons} \label{sec:gathering_photons}

            Photons are propagated from the light sources in the scene and bounced around the geometry similar to how rays are emitted through the camera viewplane and reflected/refracted based on the material properties of surfaces. In this implementation the main difference between ray and photon propagation is that the photons always terminate on the first diffuse surface which it intersects. Terminated photons are stored in a balanced kd-tree to allow for efficient photon lookup during the radiance estimation. 

            In this implementation a fixed number of photons are defined during the pre-rendering step \(P\). Each light source is assigned a number of photons based on the size of the light source.

            First the total area for all light sources in the scene is calculated: 
            \begin{align*}
            A = \sum_{l \in \mathcal{L}}{area(l)}
            \end{align*}
            where \(\mathcal{L}\) is the set of all light sources. To get the number of photons a specific light source \(l\) will emit, the contribution of \(l\) towards the total emission area is used with the total number of photons:
            \begin{align*}
            p_l = \frac{P}{A}area(l)
            \end{align*}
            which results in the number of photons a given light source should emit into the scene. Each photons' outgoing direction is cosine-weighted and the position uniformly sampled from the area of the light source. 

            \subsubsection{Radiance Estimate} \label{sec:radiance_estimate}
            In this project, the computation of the direct light is replaced by an estimation based on the photon map. To estimate the radiance at point $x$ all photons within a sphere with radius $r$. A fix sized sphere is used in this project. Some condition needs to be fulfilled in order estimate the radiance with photon mapping. If these conditions is not met the Monte-Carlo scheme will be used for the approximation. Photon mapping will not be used if the sphere contains shadow photons, or if the sphere contains few photons.

            \begin{equation*}
              L(x \rightarrow \omega_{out}) = \sum_{i=1}^M f_r(x, \omega_{in,i}, \omega_{out}) \frac{\phi(x_i \leftarrow \omega_{in,i})}{\pi r^2}
            \end{equation*}

            Areas with low photon density may give a blurry result. Therefore, a cone-filter is applied to the estimate. This is done by giving a weight $w_p$ to each photon based on the distance $d_p$ between the point $x$ and photon $p$. 
            \begin{equation*}
              w_p = max(0, 1 - d/(kr))
            \end{equation*}

            The filter is normalized by $1 - \tfrac{2}{3k}$ giving the final equation.
            \begin{equation*}
              L(x \rightarrow \omega_{out}) = \sum_{i=1}^M f_r(x, \omega_{in,i}, \omega_{out}) \frac{\phi(x_i \leftarrow \omega_{in,i}) w_p }{(1-\tfrac{2}{3k}) \pi r^2}
            \end{equation*}

        \subsection{Anti-Aliasing \& Sampling} \label{sec:anti-aliasing_and_sampling}

            Since we're \emph{sampling} irradiance falling onto pixels, we're converting from something that's \emph{continuous} (i.e. the scene description) to something \emph{discrete} (i.e. the pixels on a screen). If we're only to take one \emph{sample per pixel} (SPP) we'll most likely get multiple \emph{aliasing artifacts} (usually \emph{``jaggies''} on most edges).

            We're \emph{undersampling}, in signal processing theory. To perfectly eliminate aliasing, we'd need to fetch samples at the \emph{Nyquist rate} (or higher) and filter it. Unfortunately, that generally isn't possible, and is usually also overkill in computer graphics. Instead we use an approximation, called \emph{supersampling}, that sends importance rays from sub-pixels, and then takes the weighted average of them. Let \(p_{ij}\) be the final pixel color (unknown) at location \((i,j)\), and let \(\mathcal{S}_{ij} = \{s_1, s_2, ...\}\) be sub-pixel colors (known) we have found when raytracing. We'll estimate \(p_{ij}\) by: \[\hat{p}_{ij} = \frac{1}{|\mathcal{S}_{ij}|}\sum_{\forall s_k } s_k \; .\]

            Below you'll find an example sketch, using 4 SPP. Another important use-case of supersampling which we've failed to mention explicitly is that it replaces Monte Carlo integration over the hemisphere for the indirect diffuse  light component. We've already mentioned that we do Monte Carlo integration for direct area light sources, but in this case the Monte Carlo integration we're doing is implicit. We do this by accumulating all irradiance falling onto the pixels and then uses the above ``Monte Carlo'' estimator. Supersampling solves two big issues with one stone: anti-aliasing and also the Monte Carlo integration.

            \begin{figure}[ht]
                \centering
                \includegraphics[width=0.8\linewidth]{share/supersampling.eps}
                \caption{Intuition for \emph{supersampling} anti-aliasing technique. Average the samples \(s_1, s_2, s_3, s_4\) to find the pixel's expected color. Usually not four samples.}
                \label{fig:supersampling}
            \end{figure}

            \begin{figure}[ht]
                \centering
                \begin{subfigure}{0.32\linewidth}
                    \centering
                    \label{fig:aliasing1x}
                    \includegraphics[width=\linewidth]{share/aliasing_1x.png}
                    \caption{1x}
                \end{subfigure} \hfill
                \begin{subfigure}{0.32\linewidth}
                    \centering
                    \label{fig:aliasing4x}
                    \includegraphics[width=\linewidth]{share/aliasing_4x.png}
                    \caption{4x}
                \end{subfigure} \hfill
                \begin{subfigure}{0.32\linewidth}
                    \centering
                    \label{fig:aliasing16x}
                    \includegraphics[width=\linewidth]{share/aliasing_16x.png}
                    \caption{16x}
                \end{subfigure}
                \caption{Effects of different supersampling levels for 64x64 render of the \emph{Utah teapot} in our raytracer.}
                \label{fig:aliasing}
            \end{figure}

            Sometimes the locations where we evaluate our sub-pixels makes a difference in the final render. If the scene has of e.g. a texture which is of high-frequency (almost at the sub-pixel level) and has a regular pattern, then some details might be lost in the render process. It's largely affected by the \emph{supersampling pattern} that we use to choose the sub-pixel locations which are going to be raytraced. We've implemented three patterns, as can be seen in \cref{fig:sampling_methods}. The \emph{grid} samples at regular intervals inside the pixel with a step size \(\Delta_p\). \emph{Randomly} taking samples inside the pixel is also a valid strategy, leading to noisier, but unbiased results. Finally we have also implemented a \emph{Gaussian sampling pattern}, with distribution \(X \sim \mathcal{N}(\mu, \sigma^2)\) around the center.

            From the four vertices \(\{v_1, v_2, v_3, v_4\}\) of \(E_\Pi\) we build a coordinate system: \(\vec{x} = v_2 - v_1,\, \vec{y} = v_4 - v_1\) to calculate the next sub-pixel positions (based on the pattern we use) we're going to use to send rays.

            \begin{figure}[ht]
                \centering
                \includegraphics[width=\linewidth]{share/sampling_methods.eps}
                \caption{Visualization of \emph{supersampling patterns} we've implemented: \emph{grid}, \emph{random}, and the \emph{gaussian}. Another common technique is \emph{jittering} (not shown).}
                \label{fig:sampling_methods}
            \end{figure}

            As the last step, we need to be aware that we're storing \emph{irradiance values} (using double precision) in our pixels. They are in the so called \emph{sampling space}, but since we're going to be writing down the render to an image (integer values), we need to convert them from \emph{sample space} to \emph{image space}. We do this by \(\hat{p}_{ij} \div \max\, \{\hat{p}_{ij}\},\, \forall \hat{p}_{ij}\), scaling this down to \([0,1]\).

    \section{Results and Benchmark} \label{sec:results_and_benchmark}

        After describing our Monte Carlo raytracer we now show some example renders of the scene presented earlier in the paper, and point out the optical phenomena that arise. This raytracer has many parameters that can be toggled, and in order to identify the primary knobs that contribute to a good looking image, and those that only increase render time, we've measured the render time by changing parameters.

        \subsection*{Final Render}

        Given unconstrained time, the Monte Carlo raytracer will converge to a realistic image, and will be noise-free. After 1024 samples the image below is produced, and gives pretty convincing results, with relatively little noticeable noise. It was rendered using an \texttt{Intel Core i7 860 @ 2.8 GHz} and using \texttt{8 GiB} of on-board main memory. The raytracer supports \texttt{OpenMP}, so it was rendered in 8-way parallel, one for each hardware thread (of 4 cores).

            \begin{figure}[H]
                \centering
                \includegraphics[width=\linewidth]{share/the_render.png}
                \caption{High-resolution render of the scene at 1024x1024, 1024 SPP, 1 shadow ray and 7 bounces. Render time taken was \(\approx\) 133 minutes \& 31 seconds.}
                \label{fig:render}
            \end{figure}

            The figure above contains all optical effects required for photorealism. In the coming sections each of these effects will be shown and described in detail.

            \subsubsection*{Reflection}

            By zooming into \cref{fig:render} the mirror sphere on the right can be analyzed. We can see that we have a perfectly reflective surface, where the other surfaces in the scene, which aren't visible in the original viewplane directly, can be observed indirectly. We can see in the other figures that this can be applied recursively, as long as it doesn't go over ray depth.

            \begin{figure}[H]
                \centering
                \includegraphics[width=0.8\linewidth]{share/new_render_reflection.png}
                \label{fig:render_reflection}
            \end{figure}

            \subsubsection*{Refraction}

            Below is a sphere with perfectly refractive material using an refractive index similar to glass. The convex lens which the sphere creates causes a mirrored refracted projection. Some Fresnel effects can be observed in the sphere, i.e, we get some reflections at grazing angles. Indirect refraction of other surfaces.
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.8\linewidth]{share/new_render_refraction.png}
                \label{fig:render_refraction}
            \end{figure}

            \newpage

            \subsubsection*{Caustics}

            Caustics are usually hard to reproduce with a monte-carlo ray tracer, but given enough time to converge, there is no limitation in the method itself. In the image below we can observe caustics created by refracting direct light through the sphere, as well as those created by reflecting direct light through the mirror and then through the sphere. One possible cause for the caustics being generated underneath the sphere is the self-refraction inside the sphere. It's also able to see the caustics through the reflection on the mirror sphere. Caustics are caused because the sphere acts as a lens, concentracing light/radiance into a smaller area than normal.

            \begin{figure}[H]
                \centering
                \includegraphics[width=0.8\linewidth]{share/new_render_caustics.png}
                \label{fig:render_caustics}
            \end{figure}

            \vspace{-5em}

            \subsubsection*{Color Bleeding}

            As can be seen in the image of the white Oren-Nayar sphere, it has received indirect illumination from the red wall and the green wall. This happens because the direct light of diffuse surfaces can be reflected multiple times, and this causes the radiance to be a mix of all intersected surfaces. In the mirror reflection of this sphere, there is also contributions from the blue wall via indirect illumination. This is one of the effects which are not possible with regular Whitted raytracing and is usually an important effect of global illumination and gives photorealism. You can also see this on the walls and teapot.

            \begin{figure}[H]
                \centering
                \includegraphics[width=0.8\linewidth]{share/new_render_color_bleeding.png}
                \label{fig:render_color_bleeding}
            \end{figure}

            \subsubsection*{Photon Map Visualization}

            Below is a direct visualization of the photon map created by plotting each photon in the direct light photon map. We've chosen not to plot the scene and instead use the Cornell box since it's easier to see what's happening. The white points are regular photons carrying some flux in them, naturally each of these should carry only a little flux, so in reality they should be almost black (they are white for visualization purposes only). The black photons represent shadow photons. As can be seen, only diffuse surfaces create photons when intersected, and if there were any specular surfaces in the scene, they would only reflect or refract the photons' paths, never store them there. This was made by using \texttt{plot3d} in \emph{R} library \texttt{rgl} (see this in \texttt{utils/photon-map.r}).

            \begin{figure}[H]
                \centering
                \includegraphics[width=0.8\linewidth]{share/photon_map.png}
                \label{fig:photon_map}
            \end{figure}

        \subsection*{Parameter Benchmarks}

        In this section different parameter configurations will be tested to examine the visual and computational cost. All measurements were made on the same hardware configuration, an \texttt{Intel(R) Core(TM) i5-4250U CPU @ 1.30GHz} with 2 cores and 4 hardware threads. The system has \texttt{16 GiB} of memory. We parallelize it 4-way in \emph{OpenMP}.

            The table below shows the parameter values for our \textit{baseline} parameter configuration on which we'll perform pure Monte Carlo ray tracing, which will result in a photorealistic image given enough computation time. \cref{fig:default} gives the render of these.

            \begin{table}[H]
            \centering
            \begin{tabular}{lll}
                \toprule
                \textbf{Parameter}&\textbf{Default Value}\\
                \midrule
                Resolution&512x512\\
                Shadow Rays&1\\
                Ray Bounces&7\\
                Sampling Density&49 SPP\\
                Photon Mapping&No\\
                Photon Amount&1 million\\
                Estimation Radius &0.1 units\\
                Render Time &2 min 42 sec\\
                \bottomrule
            \end{tabular}
            \label{tab:shadow_rays}
            \end{table}

            \begin{figure}[H]
                \centering
                \caption{Render of the scene 512x512 at 49 samples per pixel, 1 shadow ray and 7 max ray bounces.}
                \includegraphics[width=0.8\linewidth]{share/results/default.png}
                \label{fig:default}
            \end{figure}

            \subsubsection*{Shadow Rays}

            This section covers how the number of shadow rays affects the result in comparison with the default parameter configuration.

            \begin{table}[H]
            \centering
            \begin{tabular}{lll}
                \toprule
                \textbf{Shadow Rays}&\textbf{Rendered}&\textbf{Time Taken}\\
                \midrule
                1&See Fig.~\ref{fig:default}&2 min 42 sec\\
                4&See Fig.~\ref{fig:shadow_rays_4}&7 min 26 sec\\
                8&See Fig.~\ref{fig:shadow_rays_8}&13 min 31 sec\\
                \bottomrule
            \end{tabular}
            \label{tab:shadow_rays}
            \end{table}

            \begin{figure}[H]
                \centering
                \caption{Render of the scene 512x512 at 49 samples per pixel, 4 shadow rays and 7 max ray bounces.}
                \includegraphics[width=0.8\linewidth]{share/results/shadow_rays_4.png}
                \label{fig:shadow_rays_4}
            \end{figure}

            \begin{figure}[H]
                \centering
                \caption{Render of the scene 512x512 at 49 samples per pixel, 8 shadow rays and 7 max ray bounces.}
                \includegraphics[width=0.8\linewidth]{share/results/shadow_rays_8.png}
                \label{fig:shadow_rays_8}
            \end{figure}

            Figure~\ref{fig:default}-\ref{fig:shadow_rays_8} have been rendered with the same resolution and samples but with increasing amount of shadow rays. By comparing the images we can see that the number of shadow rays does not affect the result. Since we sample each pixel multiple times we implicitly get multiple shadow rays. Therefore, one shadow ray is enough to get a good result and the rendering time decreases as in the table above.

            \subsubsection*{Ray-depth}

            The ray-depth parameter affects how many bounces each ray can do in the scene, with a value of 1 no reflections/refractions will occur.

            \begin{table}[H]
            \centering
            \begin{tabular}{lll}
                \toprule
                \textbf{Ray Bounces}&\textbf{Rendered}&\textbf{Time Taken}\\
                \midrule
                1&See Fig.~\ref{fig:bounce_rays_1}&51.4 sec\\
                4&See Fig.~\ref{fig:bounce_rays_4}&2 min 9 sec\\
                7&See Fig.~\ref{fig:default}&2 min 42 sec\\
                \bottomrule
            \end{tabular}
            \label{tab:ray_bounces}
            \end{table}

            \begin{figure}[H]
                \centering
                \caption{Render of the scene 512x512 at 49 samples per pixel, 1 shadow rays and 1 max ray bounces.}
                \includegraphics[width=0.8\linewidth]{share/results/bounce_rays_1.png}
                \label{fig:bounce_rays_1}
            \end{figure}

            The scene becomes much darker since no indirect illumination is present in the scene. There is also no reflection and refraction present, since those count as ray bounces as well. This is a great image for visualizing the difference between the Oren-Nayar and the Lambertian surface. It might not be noticeable, but the falloff around the Oren-Nayar reflector is a less pronounced than that of the Lambertian emitter because the roughness of it is higher.

            \begin{figure}[H]
                \centering
                \caption{Render of the scene 512x512 at 49 samples per pixel, 1 shadow rays and 4 max ray bounces.}
                \includegraphics[width=0.7\linewidth]{share/results/bounce_rays_4.png}
                \label{fig:bounce_rays_4}
            \end{figure}
            \vspace{-0.5em}

            In \cref{fig:bounce_rays_4} we can see that indirect light seems to be arriving at the points we sample, but the reflections of high depth return black. This is something which doesn't happen in the baseline configuration.

            \vspace{-0.5em}

            \subsubsection*{Sampling Density}

            The sampling density specifies how many samples will be taken for each pixel on the screen.

            \begin{table}[H]
            \centering
            \begin{tabular}{lll}
                \toprule
                \textbf{Supersamples}&\textbf{Rendered}&\textbf{Time Taken}\\
                \midrule
                16&See Fig.~\ref{fig:supersamples_16}&50.3 sec\\
                49&See Fig.~\ref{fig:default}&2 min 42 sec\\
                196&See Fig.~\ref{fig:supersamples_192}&10 min 24 sec\\
                \bottomrule
            \end{tabular}
            \label{tab:supersamples}
            \end{table}

            \begin{figure}[H]
                \centering
                \caption{Render of the scene 512x512 at 16 samples per pixel, 1 shadow ray and 7 max ray bounces.}
                \includegraphics[width=0.8\linewidth]{share/results/supersamples_16.png}
                \label{fig:supersamples_16}
            \end{figure}

            In the figure above, the scene initially seems very dark. This is expected, since not enough indirect illumination has been spread through the scene yet. It also has to do with how our raytracer does normalization of the radiance in each pixel to integer colors, something like \emph{tone mapping} might fix this.

            \begin{figure}[H]
                \centering
                \caption{Render of the scene 512x512 at 196 samples per pixel, 1 shadow ray and 7 max ray bounces.}
                \includegraphics[width=0.8\linewidth]{share/results/supersamples_196.png}
                \label{fig:supersamples_192}
            \end{figure}

            As expected the final results give less noise with more samples but at the cost of additional computation time. This also reduces aliasing and thus the jaggies which were presented earlier. The aliasing near the sources of light is especially bad at the start because it doesn't fall off as smoothly as the other surfaces in the scene. That is, it's a very sharp change in color, and it thus takes a lot of samples to blend it with the nearby colors.

            \subsubsection*{Photon Mapping}

            The following table shows how using a photon map affects the visual and computational results of the render with a fixed number of photons but different radiance estimation bound. This photon map implementation does not use k-nn photon gathering but instead uses a fixed size sphere around each  point of interest.

        \begin{table}[H]
            \centering
            \begin{tabular}{lll}
                \toprule
                \textbf{Radius}&\textbf{Rendered}&\textbf{Time Taken}\\
                \midrule
                0.1&See Fig.~\ref{fig:photons_0_1}&51.5 s\\
                0.7&See Fig.~\ref{fig:photons_0_7}&129.6 s\\
                \bottomrule
            \end{tabular}
            \label{tab:photon_mapping}
            \end{table}

            \begin{figure}[H]
                \centering
                \caption{Render of the scene 512x512 at 49 samples per pixel, 8 shadow rays and 7 max ray bounces using a photon map with 1 000 000 photons and a radiance estimation radius of 0.1.}
                \includegraphics[width=0.8\linewidth]{share/results/photons_0_1.png}
                \label{fig:photons_0_1}
            \end{figure}

            Stray shadow-photons causes the result to look spotty since the photon map and path tracer doesn't agree with the amount of radiance for a given point.

             \begin{figure}[H]
                \centering
                \caption{Render of the scene 512x512 at 49 samples per pixel, 8 shadow rays and 7 max ray bounces using a photon map with 1 000 000 photons and a radiance estimation radius of 0.7.}
                \includegraphics[width=0.8\linewidth]{share/results/photons_0_7.png}
                \label{fig:photons_0_7}
            \end{figure}

            With a larger sphere of interest the result takes significantly longer to compute but the result doesn't become as spotty as the previous configuration.

    \section{Discussion and Outlook} \label{sec:discussion_and_outlook}
    

    \newpage % Next column...
    \nocite{*} % Include all.
    \bibliographystyle{abbrv}
    \bibliography{mcrt}
\end{document}
